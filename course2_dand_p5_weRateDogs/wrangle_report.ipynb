{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangle Report\n",
    "\n",
    "### by Travis Gillespie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Gather Data](#gather)\n",
    "- [Assess Data](#assess)\n",
    "- [Clean Data](#clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to wrangle data from Twitter's tweet archive, under _WeRateDogs_. _WeRateDogs_ Twitter account rates dogs with a humourous comment. This project allowed me to apply what I learned throughout the data wrangling section, plus other skills obtained through the certification program. Some of the skills I applied are outlined below in the the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project required me to retreive three datasets using various methods.\n",
    "\n",
    "* **Twitter Archive**: Udacity provided this data in a CSV file titled *twitter_archive_enhanced*.\n",
    "----\n",
    "\n",
    "* **Tweet Image Predictions**: This data is hosted on Udacity's servers, and I downloaded a TSV file titled *image_predictions* programmatically using the Requests library and URL (provided by Udacity).\n",
    "\n",
    "----\n",
    "\n",
    "* **Twitter API**: \n",
    "I queried the Twitter API using each tweet's tweet ID (found in the Twitter archive) and Python's Tweepy library. This process takes about 15 minutes to run, so I created a progress bar to ensure the process isn't cut off at any time (_e.g. rate limit_). I also created a time delay which should also prevent the rate limit hitting it's max value. The data was returned in JSON format then stored in a TXT file titled *tweet_json*. I was then able to read the *tweet_json* file into my project as a pandas DataFrame.\n",
    "----\n",
    "\n",
    "* **Backup Copies**: \n",
    "Copies of the original pieces of data were made just before the *Assessing Data* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After retreiving each of the three datasets I assessed the data. I started with the _head()_ function to get an idea of what the data looked like, followed by _info()_ to determine both size & shape of each table. This also helped me quickly determine if there were any missing values. I also noted any potential issues and how I might go about fixing them within the _Quality_ and _Tidiness_ sections, and added anchor links to each item that could be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I joined the tables then fixed the quality and tidiness issues previously mentioned. In this project, I found dropping unwanted columns from a dataframe was more efficient and provided two advantages when compared to creating new dataframes with desired columns:\n",
    "1. There were less columns to drop (12) than there were to add (21) to a new dataframe.\n",
    "* Dropping columns maintains the datatypes for each column within the dataframe, whereas creating a new dataframe would require me to convert the date and time columns from an object to a datetime again.\n",
    "\n",
    "Some of the steps within the cleaning process were fairly straightforward. Such as removing null values (_e.g. missing images, retweets with no data_), checking for duplications, and renaming columns. However, I had to get creative with more challenging problems, as listed below:\n",
    "\n",
    "1. Some dog's had very unusual names (_e.g. '', 'a', 'my', etc._). I sorted grouped all names by the length of characters in their name. This allowed me to easily spot the most peculiar values, which I then dropped rows containing those values from the dataset.\n",
    "* I placed the developmental stage values (columns _'pupper', 'puppo', 'doggo', 'floofer'_) into one column. I had to make adustments for multiple entries. I decided to code these values with the label _multi-entry_.\n",
    "* Some of the rows within _text_ column contained decimals in reference to  rating_numerator and rating_denominator. I used a regular expression to append these values to a list. I re-ran the division calculation and added a new column to the dataframe.\n",
    "\n",
    "Finally, I wrote the clean data to a CSV file labeled *twitter_archive_master*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
